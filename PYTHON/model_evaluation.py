# -*- coding: utf-8 -*-
"""Model Evaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P8icK54Kosaz_drE3J3AJw_Zh5FPKKF1
"""

import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import Ridge, Lasso
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Load the CSV file and inspect the columns
data = pd.read_csv('/content/mtc.csv')  # Adjust the delimiter if necessary

# Strip extra spaces from column names
data.columns = data.columns.str.strip()

# If 'Item' column exists, set it as the index and transpose the DataFrame
if 'Item' in data.columns:
    data = data.set_index('Item').T
else:
    print("Error: 'Item' column not found. Check the CSV file formatting.")
    exit()

# Define features and target
features = [
    'AVERAGE AGE OF BUS',
    'NEW BUSES PUT ON ROAD',
    'KM/BUS/DAY',
    'KM EFFICIENCY %',
    'BREAKDOWNS',
    'PASSENGER/DAY (IN LAKHS)'
]
target = '% OF FLEET UTILIZATION'

# Select features and target
X = data[features]
y = data[target]

# Feature Engineering: Add polynomial features (interaction and higher-degree terms)
poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.5, random_state=42)

# Initialize models
models = {
    "Random Forest": RandomForestRegressor(n_estimators=100, random_state=42),
    "Gradient Boosting": GradientBoostingRegressor(n_estimators=100, random_state=42),
    "Ridge Regression": Ridge(alpha=1.0),
    "Lasso Regression": Lasso(alpha=0.1)
}

# Train and evaluate models
for model_name, model in models.items():
    # Cross-validation (using n_splits=2 to avoid errors with too few samples)
    cv_scores = cross_val_score(model, X_train, y_train, cv=2, scoring='neg_mean_squared_error')
    mean_cv_score = cv_scores.mean()
    print(f"{model_name} Cross-Validation MSE: {-mean_cv_score}")

    # Train the model
    model.fit(X_train, y_train)

    # Make predictions
    y_pred = model.predict(X_test)

    # Evaluate the model
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    print(f"{model_name} MSE: {mse}")
    print(f"{model_name} R-squared: {r2}")

    # Plot Actual vs Predicted for the model
    plt.figure(figsize=(8, 6))
    plt.scatter(y_test, y_pred, color='blue', label='Predicted vs Actual')
    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red',
             linestyle='--')  # Perfect prediction line
    plt.title(f'{model_name} - Actual vs Predicted')
    plt.xlabel('Actual % of Fleet Utilization')
    plt.ylabel('Predicted % of Fleet Utilization')
    plt.legend()
    plt.show()

    # Residuals Plot
    residuals = y_test - y_pred
    plt.figure(figsize=(8, 6))
    sns.histplot(residuals, kde=True, color='green')
    plt.title(f'{model_name} - Residuals Distribution')
    plt.xlabel('Residuals (Error)')
    plt.ylabel('Frequency')
    plt.show()

    # Scatter plot of residuals
    plt.figure(figsize=(8, 6))
    plt.scatter(y_pred, residuals, color='purple')
    plt.axhline(y=0, color='red', linestyle='--')
    plt.title(f'{model_name} - Residuals vs Predicted Values')
    plt.xlabel('Predicted % of Fleet Utilization')
    plt.ylabel('Residuals (Error)')
    plt.show()

# Dictionary to store results for each model
results = {}

# Train and evaluate models
for model_name, model in models.items():
    # Cross-validation (using n_splits=2 to avoid errors with too few samples)
    cv_scores = cross_val_score(model, X_train, y_train, cv=2, scoring='neg_mean_squared_error')
    mean_cv_score = cv_scores.mean()
    print(f"{model_name} Cross-Validation MSE: {-mean_cv_score}")

    # Train the model
    model.fit(X_train, y_train)

    # Make predictions
    y_pred = model.predict(X_test)

    # Evaluate the model
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    # Store results
    results[model_name] = {"MSE": mse, "R-squared": r2}

    print(f"{model_name} MSE: {mse}")
    print(f"{model_name} R-squared: {r2}")

    # Plot Actual vs Predicted for the model
    plt.figure(figsize=(8, 6))
    plt.scatter(y_test, y_pred, color='blue', label='Predicted vs Actual')
    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red',
             linestyle='--')  # Perfect prediction line
    plt.title(f'{model_name} - Actual vs Predicted')
    plt.xlabel('Actual % of Fleet Utilization')
    plt.ylabel('Predicted % of Fleet Utilization')
    plt.legend()
    plt.show()

    # Residuals Plot
    residuals = y_test - y_pred
    plt.figure(figsize=(8, 6))
    sns.histplot(residuals, kde=True, color='green')
    plt.title(f'{model_name} - Residuals Distribution')
    plt.xlabel('Residuals (Error)')
    plt.ylabel('Frequency')
    plt.show()

    # Scatter plot of residuals
    plt.figure(figsize=(8, 6))
    plt.scatter(y_pred, residuals, color='purple')
    plt.axhline(y=0, color='red', linestyle='--')
    plt.title(f'{model_name} - Residuals vs Predicted Values')
    plt.xlabel('Predicted % of Fleet Utilization')
    plt.ylabel('Residuals (Error)')
    plt.show()

# Find the best model based on lowest MSE
best_model_name = min(results, key=lambda x: results[x]['MSE'])
best_model_metrics = results[best_model_name]

print(f"\nBest Model: {best_model_name}")
print(f"MSE: {best_model_metrics['MSE']}")
print(f"R-squared: {best_model_metrics['R-squared']}")